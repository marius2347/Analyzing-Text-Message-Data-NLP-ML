{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analyzing Text Message Data - NLP ML</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset clean_nus_sms.csv\n",
    "clean_nus_sms = pd.read_csv('./clean_nus_sms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Message</th>\n",
       "      <th>length</th>\n",
       "      <th>country</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10120</td>\n",
       "      <td>Bugis oso near wat...</td>\n",
       "      <td>21</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10121</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10122</td>\n",
       "      <td>I dunno until when... Lets go learn pilates...</td>\n",
       "      <td>46</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10123</td>\n",
       "      <td>Den only weekdays got special price... Haiz......</td>\n",
       "      <td>140</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10124</td>\n",
       "      <td>Meet after lunch la...</td>\n",
       "      <td>22</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                            Message  \\\n",
       "0           0  10120                              Bugis oso near wat...   \n",
       "1           1  10121  Go until jurong point, crazy.. Available only ...   \n",
       "2           2  10122     I dunno until when... Lets go learn pilates...   \n",
       "3           3  10123  Den only weekdays got special price... Haiz......   \n",
       "4           4  10124                             Meet after lunch la...   \n",
       "\n",
       "  length country    Date  \n",
       "0     21      SG  2003/4  \n",
       "1    111      SG  2003/4  \n",
       "2     46      SG  2003/4  \n",
       "3    140      SG  2003/4  \n",
       "4     22      SG  2003/4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_nus_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Message</th>\n",
       "      <th>length</th>\n",
       "      <th>country</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10120</td>\n",
       "      <td>Bugis oso near wat...</td>\n",
       "      <td>21</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10121</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10122</td>\n",
       "      <td>I dunno until when... Lets go learn pilates...</td>\n",
       "      <td>46</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10123</td>\n",
       "      <td>Den only weekdays got special price... Haiz......</td>\n",
       "      <td>140</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10124</td>\n",
       "      <td>Meet after lunch la...</td>\n",
       "      <td>22</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                            Message  \\\n",
       "0           0  10120                              Bugis oso near wat...   \n",
       "1           1  10121  Go until jurong point, crazy.. Available only ...   \n",
       "2           2  10122     I dunno until when... Lets go learn pilates...   \n",
       "3           3  10123  Den only weekdays got special price... Haiz......   \n",
       "4           4  10124                             Meet after lunch la...   \n",
       "\n",
       "  length country    Date  \n",
       "0     21      SG  2003/4  \n",
       "1    111      SG  2003/4  \n",
       "2     46      SG  2003/4  \n",
       "3    140      SG  2003/4  \n",
       "4     22      SG  2003/4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also make a copy\n",
    "clean_nus_sms_copy = clean_nus_sms.copy()\n",
    "\n",
    "clean_nus_sms_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_Tokenized_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bugis, oso, near, wat, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, dunno, until, when, ..., Lets, go, learn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Den, only, weekdays, got, special, price, ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Meet, after, lunch, la, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Words_Tokenized_Message\n",
       "0                       [Bugis, oso, near, wat, ...]\n",
       "1  [Go, until, jurong, point, ,, crazy, .., Avail...\n",
       "2  [I, dunno, until, when, ..., Lets, go, learn, ...\n",
       "3  [Den, only, weekdays, got, special, price, ......\n",
       "4                      [Meet, after, lunch, la, ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "# 1) Tokenization\n",
    "\n",
    "def tokenize_words(text):\n",
    "    if isinstance(text, str):\n",
    "        sentences = word_tokenize(text)\n",
    "    else:\n",
    "        sentences = []\n",
    "    return sentences\n",
    "\n",
    "new_clean_nus_sms = pd.DataFrame()\n",
    "new_clean_nus_sms['Words_Tokenized_Message'] = clean_nus_sms_copy['Message'].apply(tokenize_words)\n",
    "new_clean_nus_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:311: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_Tokenized_Message</th>\n",
       "      <th>Noise_Removed_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bugis, oso, near, wat, ...]</td>\n",
       "      <td>Bugis oso near wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, dunno, until, when, ..., Lets, go, learn, ...</td>\n",
       "      <td>dunno Lets go learn pilates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Den, only, weekdays, got, special, price, ......</td>\n",
       "      <td>Den weekdays got special price Haiz Cant eat l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Meet, after, lunch, la, ...]</td>\n",
       "      <td>Meet lunch la</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Words_Tokenized_Message  \\\n",
       "0                       [Bugis, oso, near, wat, ...]   \n",
       "1  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "2  [I, dunno, until, when, ..., Lets, go, learn, ...   \n",
       "3  [Den, only, weekdays, got, special, price, ......   \n",
       "4                      [Meet, after, lunch, la, ...]   \n",
       "\n",
       "                               Noise_Removed_Message  \n",
       "0                                 Bugis oso near wat  \n",
       "1  Go jurong point crazy Available bugis n great ...  \n",
       "2                        dunno Lets go learn pilates  \n",
       "3  Den weekdays got special price Haiz Cant eat l...  \n",
       "4                                      Meet lunch la  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Noise removal\n",
    "\n",
    "def preprocess_text(words):\n",
    "    if not words:  \n",
    "        return ''\n",
    "    \n",
    "    # a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # remove punctuation and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return processed_text\n",
    "new_clean_nus_sms['Noise_Removed_Message'] = new_clean_nus_sms['Words_Tokenized_Message'].apply(preprocess_text)\n",
    "new_clean_nus_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marius/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_Tokenized_Message</th>\n",
       "      <th>Noise_Removed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Stemmed_Tokens</th>\n",
       "      <th>Lemmatized_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bugis, oso, near, wat, ...]</td>\n",
       "      <td>Bugis oso near wat</td>\n",
       "      <td>[Bugis, oso, near, wat]</td>\n",
       "      <td>[bugi, oso, near, wat]</td>\n",
       "      <td>[Bugis, oso, near, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, dunno, until, when, ..., Lets, go, learn, ...</td>\n",
       "      <td>dunno Lets go learn pilates</td>\n",
       "      <td>[dunno, Lets, go, learn, pilates]</td>\n",
       "      <td>[dunno, let, go, learn, pilat]</td>\n",
       "      <td>[dunno, Lets, go, learn, pilate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Den, only, weekdays, got, special, price, ......</td>\n",
       "      <td>Den weekdays got special price Haiz Cant eat l...</td>\n",
       "      <td>[Den, weekdays, got, special, price, Haiz, Can...</td>\n",
       "      <td>[den, weekday, got, special, price, haiz, cant...</td>\n",
       "      <td>[Den, weekday, got, special, price, Haiz, Cant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Meet, after, lunch, la, ...]</td>\n",
       "      <td>Meet lunch la</td>\n",
       "      <td>[Meet, lunch, la]</td>\n",
       "      <td>[meet, lunch, la]</td>\n",
       "      <td>[Meet, lunch, la]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Words_Tokenized_Message  \\\n",
       "0                       [Bugis, oso, near, wat, ...]   \n",
       "1  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "2  [I, dunno, until, when, ..., Lets, go, learn, ...   \n",
       "3  [Den, only, weekdays, got, special, price, ......   \n",
       "4                      [Meet, after, lunch, la, ...]   \n",
       "\n",
       "                               Noise_Removed_Message  \\\n",
       "0                                 Bugis oso near wat   \n",
       "1  Go jurong point crazy Available bugis n great ...   \n",
       "2                        dunno Lets go learn pilates   \n",
       "3  Den weekdays got special price Haiz Cant eat l...   \n",
       "4                                      Meet lunch la   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0                            [Bugis, oso, near, wat]   \n",
       "1  [Go, jurong, point, crazy, Available, bugis, n...   \n",
       "2                  [dunno, Lets, go, learn, pilates]   \n",
       "3  [Den, weekdays, got, special, price, Haiz, Can...   \n",
       "4                                  [Meet, lunch, la]   \n",
       "\n",
       "                                      Stemmed_Tokens  \\\n",
       "0                             [bugi, oso, near, wat]   \n",
       "1  [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
       "2                     [dunno, let, go, learn, pilat]   \n",
       "3  [den, weekday, got, special, price, haiz, cant...   \n",
       "4                                  [meet, lunch, la]   \n",
       "\n",
       "                                   Lemmatized_Tokens  \n",
       "0                            [Bugis, oso, near, wat]  \n",
       "1  [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "2                   [dunno, Lets, go, learn, pilate]  \n",
       "3  [Den, weekday, got, special, price, Haiz, Cant...  \n",
       "4                                  [Meet, lunch, la]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Text normalization\n",
    "\n",
    "# normalize \n",
    "def tokenize_and_normalize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# stemming\n",
    "def stem_tokens(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(token) for token in tokens]\n",
    "    return stemmed\n",
    "\n",
    "# lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "new_clean_nus_sms['Tokens'] = new_clean_nus_sms['Noise_Removed_Message'].apply(tokenize_and_normalize)\n",
    "new_clean_nus_sms['Stemmed_Tokens'] = new_clean_nus_sms['Tokens'].apply(stem_tokens)\n",
    "new_clean_nus_sms['Lemmatized_Tokens'] = new_clean_nus_sms['Tokens'].apply(lemmatize_tokens)\n",
    "new_clean_nus_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [Bugis, oso, near, wat]\n",
       "1        [Go, jurong, point, crazy, Available, bugis, n...\n",
       "2                         [dunno, Lets, go, learn, pilate]\n",
       "3        [Den, weekday, got, special, price, Haiz, Cant...\n",
       "4                                        [Meet, lunch, la]\n",
       "                               ...                        \n",
       "48593                                         [Come, NOON]\n",
       "48594                                               [LOVE]\n",
       "48595                                                [CYA]\n",
       "48596                                              [GUEST]\n",
       "48597                           [MANY, MANY, MANY, PEOPLE]\n",
       "Name: Lemmatized_Tokens, Length: 48598, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct analysis data\n",
    "X = new_clean_nus_sms['Lemmatized_Tokens']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct sentiment analysis on the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common topics users text about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how similar various users’ texts are (by topic, vocabulary use, or Levenshtein distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what kinds of biases exist in the text message data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distributions for messaging, sentiments, or key words/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use POS tagging to find commonly used phrases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
